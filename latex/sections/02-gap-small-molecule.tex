\section{Gap 1: The Small Molecule Bias}
\label{sec:small-molecule}

Current agentic AI systems for drug discovery are architected around small molecules. The tools they integrate, the data formats they expect, and the workflows they support all reflect an implicit assumption that therapeutic candidates will be represented as SMILES strings, evaluated through docking scores and synthetic accessibility metrics, and optimized via retrosynthesis planning. This design works well for traditional medicinal chemistry but breaks down completely for peptide therapeutics, which require fundamentally different computational approaches.

\subsection{The Peptide-Specific Challenge Space}

Therapeutic peptides occupy a distinct region of chemical space. Composed of 2 to 50 amino acid residues, they bridge small molecules and biologics, combining drug-like properties with biomolecular complexity. Unlike small molecules with rigid scaffolds and well-defined conformations, peptides are conformationally flexible, sampling diverse structural states in solution. This flexibility creates both opportunities and challenges: peptides can achieve exquisite target selectivity through induced-fit binding, but they also face aggregation, protease degradation, and membrane permeability barriers that small molecules rarely encounter.

The discovery workflow for peptides diverges fundamentally from small-molecule medicinal chemistry. Structure-activity relationships learned from small-molecule campaigns do not transfer; a conservative amino acid substitution can abolish activity, while a seemingly drastic change might improve potency tenfold. Stability becomes the dominant constraint. In our experience developing peptides for traumatic brain injury treatment, the efficacy-stability trade-off was more critical than potency optimization. A highly bioactive peptide with a serum half-life of minutes has no therapeutic value. Protease resistance requires modeling peptide-enzyme interactions across dozens of serine, cysteine, and metalloprotease families. Immunogenicity prediction demands epitope scanning and MHC binding prediction. Aggregation propensity depends on sequence charge distribution, hydrophobic patterning, and secondary structure preferences.

Current agent systems provide no pathway to address these peptide-specific requirements. ChemCrow's tool suite includes RDKit for molecular property calculation, but RDKit does not handle peptide conformational sampling. PharmAgents integrates docking tools assuming rigid-body ligand-receptor interactions, inappropriate for flexible peptides. ChatInvent mines literature for small-molecule synthesis routes, irrelevant to peptide synthesis via solid-phase peptide synthesis or recombinant expression. The architectural assumption is that candidates can be represented as SMILES strings and evaluated through cheminformatics libraries designed for druglike molecules.

When practitioners attempt to repurpose these systems for peptide design, they encounter immediate friction. Peptide sequences cannot be encoded as SMILES without losing critical information about stereochemistry and modifications. Molecular fingerprints like Morgan fingerprints or MACCS keys, central to small-molecule similarity searches and QSAR models, have no natural analog for peptides. Docking protocols optimized for rigid small molecules produce unreliable scores for conformationally flexible peptides. Synthetic accessibility metrics based on retrosynthesis rules are meaningless when synthesis follows entirely different chemistry.

\subsection{Protein Language Models vs Molecular Fingerprints}

The solution space for peptide design is defined by protein language models, not cheminformatics. Models like ProtBERT \citep{elnaggar2021protbert}, ESM-2 \citep{lin2023esm2}, and ProGen \citep{madani2023progen} encode evolutionary and structural priors learned from millions of natural protein sequences. These models capture sequence-structure-function relationships that cannot be derived from first principles or small-molecule analogy. ESM-2 embeddings enable receptor type prediction from sequence alone, identifying which G-protein coupled receptor subtypes a peptide might engage without requiring structural modeling. ProtBERT fine-tuning supports transfer learning from general protein sequence corpora to domain-specific bioactivity prediction tasks with fewer than 100 training examples.

In practice, building peptide discovery workflows around protein language models requires capabilities that current agent architectures do not provide. Consider a representative task: developing a classifier to predict whether a peptide will bind a specific receptor family. The workflow involves curating a training set of positive and negative examples, extracting ESM-2 embeddings for each sequence, training a supervised classifier on these embeddings, validating performance on held-out data, and iterating on model architecture and hyperparameters. This is a gradient-based machine learning task, not an API call to a pre-trained model.

No current agent can execute this workflow autonomously. The LLM orchestrator paradigm assumes that models are black boxes accessed via inference APIs. There is no support for fine-tuning, no mechanism to version control training datasets, no workflow for hyperparameter search. An agent might retrieve an ESM-2 embedding via an API, but it cannot adjust the model's attention heads to emphasize binding-relevant sequence motifs or train a task-specific classifier on proprietary data.

The disconnect extends to generative modeling. Practitioners use models like ProtGPT2 \citep{ferruz2022protgpt2} for de novo peptide generation, fine-tuning on therapeutic sequences to bias generation toward bioactive regions of sequence space. Training ProtGPT2 involves defining a tokenization scheme, constructing batches of sequences with appropriate padding and masking, computing next-token prediction losses, and optimizing via gradient descent. Reinforcement learning-based approaches, which we have used to optimize peptides for multi-objective reward functions combining bioactivity and stability, require defining a reward model, sampling from a policy network, computing policy gradients, and updating the policy with KL regularization to prevent mode collapse. These are not tasks that LLM agents orchestrating external tools can perform. They are machine learning workflows requiring end-to-end control over model training.

The architectural gap is not a missing tool; it is a missing paradigm. Protein language models are not accessories to small-molecule workflows. They are the foundation of peptide discovery, requiring first-class support for model training, fine-tuning, and integration with downstream analysis pipelines.

\subsection{What Peptide-Aware Agents Would Need}

A peptide-aware agent architecture would treat protein language models as core components, not external APIs. This requires several capabilities absent from current systems. First, support for model fine-tuning pipelines: dataset curation from proprietary assays, train-validation-test splitting, learning rate scheduling, early stopping, and model checkpointing. The agent should be able to take a pre-trained ProtBERT model, fine-tune it on 200 peptide sequences with bioactivity labels, and return a calibrated classifier with uncertainty estimates.

Second, integration with structural biology tools as architectural primitives. AlphaFold \citep{jumper2021alphafold} structure prediction is not a side calculation but a central step in peptide design. Flexible docking with conformational sampling requires different protocols than small-molecule docking. Molecular dynamics simulations provide insights into peptide stability and binding kinetics that cannot be inferred from static structures. These tools must be callable within multi-step workflows, with intermediate results feeding into subsequent steps.

Third, multi-objective sequence optimization. Peptide design inherently involves trade-offs: bioactivity versus stability, potency versus selectivity, efficacy versus immunogenicity. Practitioners employ reinforcement learning, Monte Carlo sampling, and genetic algorithms to explore these trade-off spaces. In our experience optimizing peptides for in vivo efficacy, we used curriculum learning: initially rewarding any improvement in bioactivity, then progressively adding stability and toxicity constraints as the policy network learned. This curriculum structure prevented premature convergence to local optima and maintained sequence diversity. Current agents provide no framework for defining such multi-stage optimization processes.

Fourth, diversity-aware generation. Generative models for peptides often suffer from mode collapse, repeatedly producing sequences that maximize the reward function but offer little structural variety. Practitioners use diversity penalties, such as penalizing generated sequences that are too similar to previously generated ones, or max-min diversity rewards that balance novelty and predicted activity. These strategies require maintaining state across generation steps, computing sequence similarity metrics in embedding space, and adjusting sampling probabilities dynamically. LLM-centric agents, which treat each tool call as stateless, cannot implement such strategies.

Finally, active learning loops for experimental validation. With limited synthesis and testing budgets, practitioners must carefully select which peptides to validate. Active learning selects candidates that maximize information gain, prioritizing sequences where the model's predictions are most uncertain or where validation would most improve the model. This requires uncertainty quantification, acquisition function optimization, and feedback loops where experimental results update model parameters. The loop must operate across multiple rounds: generate candidates, select via active learning, obtain experimental data, retrain model, repeat.

The peptide discovery workflow is not a variant of small-molecule discovery with different tools. It is a fundamentally different computational paradigm, requiring machine learning training, structural biology integration, and multi-objective optimization as core capabilities. Until agent architectures recognize and support these requirements, their utility for peptide therapeutics will remain severely limited. The small-molecule bias is not merely a gap in tool coverage; it reflects architectural assumptions that must be revisited to accommodate the diversity of therapeutic modalities in modern drug discovery.
