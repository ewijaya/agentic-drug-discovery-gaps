\section{Introduction: The Promise vs Reality of Agentic AI in Drug Discovery}
\label{sec:introduction}

\subsection{The Current Narrative}

Recent agentic AI systems demonstrate impressive capabilities. Coscientist autonomously plans chemical syntheses \citep{boiko2023coscientist}, ChemCrow orchestrates 18 chemistry tools \citep{bran2023chemcrow}, and ChatInvent completed a 13-month deployment at AstraZeneca for literature synthesis \citep{he2026chatinvent}. PharmAgents integrates knowledge graphs for target identification \citep{swanson2024pharmagents}, while MADD and DiscoVerse promise multi-agent collaboration. The dominant narrative positions agentic AI as the next frontier, moving beyond static models to systems that autonomously navigate literature, design experiments, and propose hypotheses \citep{lakhan2025agentic, seal2025aiagents}.

The architectural pattern is consistent: a large language model orchestrates tool calls, synthesizes results, and generates explanations. ChemCrow routes requests to RDKit, PubChem, and reaction prediction APIs. ChatInvent mines literature for research gaps. Coscientist interfaces with laboratory automation. This LLM-centric design works for text-based reasoning tasks: literature review, synthesis enumeration, protocol documentation, and safety analysis. The enthusiasm is warranted, but it is also narrowly scoped. Most systems are optimized for small-molecule workflows, high-throughput in vitro assays, and organizations with large datasets and extensive compute. When those assumptions break, performance degrades in ways the demos do not reveal. This is the practical gap practitioners encounter in day-to-day work.
\subsection{The Gap: What Happens Outside the Lab Automation Paradigm}

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{figures/fig1-agent-reality-gap.pdf}
\caption{The Agent Reality Gap in Drug Discovery. Left panel shows computational workflows where current AI agents excel: small molecule representations (SMILES strings), databases, literature mining, and virtual screening. Right panel depicts the messy reality of drug discovery: multi-modal biological data from animal studies, wet lab iteration, and multi-objective trade-offs. The gap between these contexts represents the architectural limitations addressed in this paper.}
\label{fig:agent-reality-gap}
\end{figure}

However, these systems reveal systematic blind spots outside their design context: small-molecule discovery at well-resourced pharmaceutical companies. Peptide therapeutics require protein language models like ESM-2 \citep{lin2023esm2} or ProtBERT \citep{elnaggar2021protbert}, not molecular fingerprints. Peptides (5 to 50 amino acids) have complex conformational dynamics, aggregation propensities, and protease vulnerabilities absent in small molecules. No current agent supports protein language model fine-tuning, conformational sampling, or aggregation prediction.

In vivo efficacy studies generate longitudinal, multi-modal data: behavioral scores over weeks, tissue histology, RNA sequencing, and clinical notes. In traumatic brain injury models, efficacy manifests as motor coordination improvements at day 7, reduced neuroinflammation at day 14, and neurogenesis at day 28. No agent integrates these temporal data streams for outcome prediction. The result is a gap between in vitro promise and in vivo reality, where most development cost and risk actually sit.

Small biotechs face different constraints than AstraZeneca: 50 to 500 proprietary sequences versus millions, single GPU versus clusters, one person handling design, modeling, and analysis. Transfer learning and few-shot adaptation are essential, not optional. Current agents assume abundant resources and long, interactive cycles that do not match small-team workflows.

Real drug discovery navigates multi-objective trade-offs under uncertainty. A peptide with tenfold higher bioactivity may have narrower safety margins or reduced stability. Current agents optimize single metrics or weighted sums, ignoring Pareto frontiers and uncertainty quantification. Practitioners end up doing this reasoning manually, which slows iteration and increases decision risk.

This paper draws on 14+ AI-driven projects spanning peptide design, reinforcement learning optimization, in vivo efficacy modeling, behavioral phenotyping via computer vision, RNA-seq analysis, and multi-objective navigation. Each revealed architectural assumptions that do not generalize and where pragmatic workarounds were required to get results on real timelines. These experiences anchor the critique that follows.
The following sections identify five critical gaps: small molecule bias (\S\ref{sec:small-molecule}), absence of in vivo to in silico bridges (\S\ref{sec:invivo}), LLM-centric orchestration limitations (\S\ref{sec:multiparadigm}), mismatch with small biotech realities (\S\ref{sec:smallbiotech}), and single-metric optimization (\S\ref{sec:multiobjective}). We conclude with design principles for next-generation agents (\S\ref{sec:wishlist}).
