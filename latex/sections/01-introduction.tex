\section{Introduction}
\label{sec:introduction}

\subsection{The Current Landscape}

Recent agentic AI systems have made tangible progress. Coscientist autonomously plans chemical syntheses \citep{boiko2023coscientist}, ChemCrow orchestrates 18 chemistry tools \citep{bran2024chemcrow}, and ChatInvent completed a deployment at AstraZeneca for molecular design and synthesis planning \citep{he2026chatinvent}. PharmAgents integrates knowledge graphs for target identification \citep{gao2025pharmagents}, TxGemma provides therapeutics-focused language understanding \citep{txgemma2025}, while MADD \citep{madd2025} and DiscoVerse \citep{discoverse2025} promise multi-agent collaboration. The dominant narrative positions agentic systems as the next major advance, moving beyond static models to systems that autonomously navigate literature, design experiments, and propose hypotheses \citep{lakhan2025agentic, seal2025aiagents}.

The architectural pattern is consistent: a large language model orchestrates tool calls, synthesizes results, and generates explanations. ChemCrow routes requests to RDKit, PubChem, and reaction prediction APIs. ChatInvent generates molecular designs informed by literature. Coscientist interfaces with laboratory automation. This LLM-centric design works for text-based reasoning tasks: literature review, synthesis enumeration, protocol documentation, and safety analysis. However, these demonstrations are narrowly scoped to specific contexts. Most systems are optimized for small-molecule workflows, high-throughput in vitro assays, and organizations with large datasets and extensive compute. When those assumptions break, performance degrades in ways the demos do not reveal.

An important distinction underlies the analysis that follows. Individual tools addressing aspects of each gap are emerging: peptide-aware generative models, multi-objective optimizers, and omics analysis platforms exist as standalone capabilities. The capability gaps we identify are not the absence of individual tools but the absence of agentic \textit{workflow integration} that chains these capabilities into end-to-end pipelines supporting iterative design-test cycles, proprietary data, and human-in-the-loop decision-making.
To test whether this bias extends to the foundation models themselves, we probe four
frontier LLMs on matched small-molecule and peptide questions as a diagnostic: all four
models demonstrate competent peptide reasoning, isolating the bottleneck to agent
architecture rather than model capability (\S\ref{sec:knowledge-probing}).

\subsection{Scope and Motivation}

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{figures/fig1-agent-reality-gap.jpeg}
\caption{The Agent Reality Gap in Drug Discovery. Left panel shows computational workflows where current agents excel: small molecule representations (SMILES strings), databases, literature mining, and virtual screening. Right panel depicts the messy reality of drug discovery: multi-modal biological data from animal studies, wet lab iteration, and multi-objective trade-offs. The gap between these contexts represents the architectural limitations addressed in this paper.}
\label{fig:agent-reality-gap}
\end{figure}

However, these systems reveal systematic capability gaps outside their design context: small-molecule discovery at well-resourced pharmaceutical companies. Peptide therapeutics require protein language models like ESM-2 \citep{lin2023esm2} or ProtBERT \citep{elnaggar2022protbert}, not molecular fingerprints. Peptides (5 to 50 amino acids) have complex conformational dynamics, aggregation propensities, and protease vulnerabilities absent in small molecules. No current agent supports protein language model fine-tuning, conformational sampling, or aggregation prediction.

In vivo efficacy studies generate longitudinal, multi-modal data: behavioral scores over weeks, tissue histology, RNA sequencing, and clinical notes. In neurological injury models, efficacy manifests through staged recovery endpoints: early motor improvements, subsequent reduction in neuroinflammation, and longer-term neurogenesis. No agent integrates these temporal data streams for outcome prediction. The result is a gap between in vitro promise and in vivo reality, where most development cost and risk actually sit.

Small biotechs face different constraints than AstraZeneca: 50 to 500 proprietary sequences versus millions, single GPU versus clusters, one person handling design, modeling, and analysis. Transfer learning and few-shot adaptation are prerequisites for workflows with 50-500 proprietary sequences. Current agents assume abundant resources and long, interactive cycles that do not match small-team workflows.

Real drug discovery navigates multi-objective trade-offs under uncertainty. A peptide with tenfold higher bioactivity may have narrower safety margins or reduced stability. Current agents optimize single metrics or weighted sums, ignoring Pareto frontiers and uncertainty quantification. Practitioners end up doing this reasoning manually, which slows iteration and increases decision risk.

This paper presents a systematic gap analysis drawing on over a dozen computational projects spanning peptide design, reinforcement learning optimization, in vivo efficacy modeling, behavioral phenotyping via computer vision, RNA-seq analysis, and multi-objective navigation, led by the author at a small biotech serving as both drug designer and computational practitioner. We evaluate six agentic frameworks (Table~\ref{tab:frameworks}) against 15 task classes derived from practitioner workflows (\S\ref{sec:methods}), introducing a capability matrix across five evaluation dimensions. Our analysis reveals five critical capability gaps: small-molecule representation bias (\S\ref{sec:small-molecule}), absence of in vivo-in silico integration (\S\ref{sec:invivo}), limited computational paradigm support (\S\ref{sec:multiparadigm}), misalignment with small-biotech constraints (\S\ref{sec:smallbiotech}), and single-objective optimization assumptions (\S\ref{sec:multiobjective}). From these gaps, we derive design requirements for next-generation frameworks (\S\ref{sec:requirements}). These gaps are analytically distinct but practically intertwined: multi-paradigm orchestration (Gap 3) is a prerequisite for peptide-aware workflows (Gap 1), and multi-objective reasoning (Gap 5) is needed for both in vivo translation (Gap 2) and resource-constrained decision-making (Gap 4). We present them separately to clarify the architectural requirements, while recognizing that solutions must address them jointly.
