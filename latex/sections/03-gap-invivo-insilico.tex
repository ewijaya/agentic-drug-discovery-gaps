\section{Gap 2: The In Vivo to In Silico Bridge}
\label{sec:invivo}

If the small-molecule bias limits agent utility for peptide discovery, the absence of in vivo modeling capabilities represents an even more fundamental gap. Current agentic AI systems excel at in vitro automation: Coscientist can plan chemical syntheses and execute them via laboratory robotics \citep{boiko2023coscientist}, ChemCrow can screen virtual compound libraries against protein targets, and ChatInvent can mine literature for structure-activity relationships. But drug discovery does not end at the in vitro assay. The critical validation happens in vivo, where therapeutic candidates confront the full complexity of living organisms: pharmacokinetics, biodistribution, metabolism, multi-organ toxicology, and long-term efficacy that cannot be predicted from binding affinity alone.

Animal studies generate data of a fundamentally different character than in vitro screens. They are longitudinal, spanning days to months. They are multi-modal, integrating behavioral scores, clinical observations, tissue imaging, and molecular profiling. They are noisy, with biological variability that dwarfs the precision of plate-based assays. They are low-throughput, testing tens of compounds rather than thousands. And they are expensive, consuming significant time and resources per data point. These characteristics make in vivo modeling the bottleneck in therapeutic development, yet current agent architectures provide no pathway to incorporate in vivo data into decision-making.

\subsection{The Lab Automation Ceiling}

The lab automation paradigm that drives much current enthusiasm for agentic AI reaches a hard ceiling at in vivo studies. Automated synthesis platforms like those integrated with Coscientist can prepare compounds at scale. High-throughput screening robots can test thousands of compounds per day in biochemical or cell-based assays. The entire pipeline from hypothesis to in vitro validation can, in principle, be automated. But animal experiments cannot be scaled, parallelized, or fully automated. They require specialized facilities, trained personnel, ethical oversight, and time measured in weeks, not hours.

This mismatch creates a critical gap. An agent might autonomously design a peptide, predict its binding affinity via docking, and even suggest a synthesis route. But it has no mechanism to integrate the results of a 28-day efficacy study in a traumatic brain injury model, where the primary endpoints are behavioral recovery, histological evidence of tissue regeneration, and transcriptomic signatures of neuroprotection. The data formats, temporal structure, and statistical analysis requirements are entirely outside the scope of current agent capabilities.

In practice, in vivo studies yield heterogeneous data streams. Consider a representative study evaluating a therapeutic peptide for neurological injury. Behavioral assessments measure motor coordination through tasks like beam walking or grid navigation, generating ordinal scores tracked over multiple weeks. Tissue histology captures cell proliferation, tissue architecture, and pathological markers through imaging, producing spatial data requiring computer vision analysis. RNA sequencing profiles transcriptomic responses, generating high-dimensional gene expression matrices that must be analyzed via differential expression, pathway enrichment, and upstream regulator analysis. Clinical observations document weight changes, adverse events, and qualitative health indicators in semi-structured notes. Integrating these modalities to assess therapeutic efficacy is not a task current agents can perform.

We have experienced this gap directly in developing composite efficacy metrics for in vivo peptide evaluation. Early-stage bioactivity assays might show that a peptide increases cell proliferation threefold in vitro. But in vivo, efficacy manifests through temporal dynamics: an initial inflammatory response at days 1 to 3, peak neural progenitor proliferation at days 7 to 10, and long-term functional recovery measurable at day 28. Predicting which in vitro hits will show sustained in vivo benefit requires temporal modeling that no current agent supports. We developed regression models mapping in vitro bioactivity profiles and early in vivo markers to long-term outcomes, but this workflow, dataset curation, feature engineering, model training, and validation, lies entirely outside the scope of LLM tool-calling architectures.

\subsection{Multi-Modal, Longitudinal Data Integration}

The core challenge is that in vivo data does not fit the tidy CSV format that machine learning pipelines expect. Behavioral scores are ordinal and subject to inter-rater variability. Imaging data is spatial, requiring segmentation, feature extraction, and quality control. Transcriptomic data is high-dimensional, with thousands of genes measured across multiple biological replicates and timepoints. Clinical notes are text, containing information that may be critical but is not captured in structured fields. Each modality demands specialized analysis pipelines, and the integration across modalities requires domain expertise that cannot be encoded in simple tool APIs.

Behavioral phenotyping illustrates the complexity. To quantify social bonding in rodent models, practitioners use computer vision tools like DeepLabCut \citep{mathis2018deeplabcut} to track animal poses in video recordings. This generates time-series data of keypoint coordinates: nose, ears, tail base, limbs. From these trajectories, one computes behavioral metrics: inter-animal distance over time, time spent in contact, approach-avoidance patterns, and grooming behaviors. The analysis requires training pose estimation models on labeled frames, validating tracking quality, computing derived features, and applying statistical tests to identify treatment effects. No current agent can autonomously execute this workflow. It involves video data processing, supervised learning for pose estimation, time-series feature engineering, and statistical hypothesis testing, spanning multiple computational paradigms that LLM-centric orchestration cannot coordinate.

RNA sequencing analysis presents similar challenges. Raw sequencing reads must be quality-controlled, aligned to a reference genome, and quantified into gene expression matrices. Differential expression analysis identifies genes whose expression changes significantly between treatment and control groups, accounting for multiple testing and biological variability. Pathway enrichment analysis maps differentially expressed genes to biological processes and signaling cascades, using databases like KEGG \citep{kanehisa2023kegg} or Gene Ontology. Upstream regulator analysis infers which transcription factors or signaling molecules might drive the observed expression changes. The entire pipeline, from FASTQ files to mechanistic hypotheses, requires bioinformatics tools that current agents do not integrate: alignment software (STAR, HISAT2), statistical analysis packages (DESeq2, edgeR), and pathway analysis platforms (GSEA \citep{subramanian2005gsea}, Ingenuity Pathway Analysis).

Integrating these heterogeneous data sources to identify predictive biomarkers or mechanistic signatures is perhaps the most valuable, and most absent, capability. In developing peptides for tissue regeneration, we sought to correlate in vitro bioactivity with in vivo efficacy outcomes. This required extracting features from multiple assays (proliferation, migration, secretion of growth factors), normalizing them across different measurement scales, aligning them with temporal in vivo data (behavioral scores at days 3, 7, 14, 28), and training regression models to predict long-term outcomes from short-term markers. The workflow involved feature engineering, imputation for missing data, cross-validation with stratified sampling, and model selection balancing predictive accuracy and interpretability. These are machine learning workflows, not LLM reasoning tasks.

\subsection{Safety, Efficacy, and the Translation Valley of Death}

In vivo models also surface safety-efficacy trade-offs that in silico screens miss entirely. A peptide that increases bioactivity tenfold in cell culture might trigger immune activation, hepatotoxicity, or off-target effects in animals. Conversely, modifications that improve proteolytic stability, such as incorporating D-amino acids or cyclization, may reduce target affinity or increase aggregation propensity. The optimal candidate balances these competing objectives, and the balance point depends on the therapeutic context: indications with high unmet medical need tolerate greater risk, while chronic treatments demand exceptional safety profiles.

Toxicology modeling requires dose-response analysis, modeling how adverse effects scale with peptide concentration. Generalized linear mixed models account for repeated measures within subjects, inter-individual variability, and time-dependent effects. Identifying therapeutic windows, dose ranges where efficacy plateaus but toxicity remains acceptable, is essential for clinical translation. Yet current agents provide no support for these analyses. They cannot construct dose-response curves, compute confidence intervals on LD50 estimates, or visualize therapeutic indices.

Species translation compounds the uncertainty. Pharmacokinetic parameters measured in mice must be extrapolated to rats, primates, and humans using allometric scaling laws that account for differences in body weight, metabolic rate, and organ perfusion. Peptide stability, driven by protease activity, varies across species due to differences in protease expression profiles. Immune responses show species-specific patterns; a peptide well-tolerated in rodents may provoke antibody responses in primates. Quantifying these uncertainties and propagating them into clinical dose predictions requires mechanistic modeling that current agents do not support.

In our experience navigating these trade-offs, the most valuable computational support would not be autonomous decision-making but tools for uncertainty quantification, sensitivity analysis, and Pareto frontier exploration. Given a set of candidate peptides with predicted efficacy and safety profiles, each with associated confidence intervals, what is the set of Pareto-optimal choices? How sensitive are the rankings to assay variability or model uncertainty? Which additional experiments would most reduce decision uncertainty? These questions require Bayesian optimization, multi-objective decision theory, and value-of-information analysis, none of which fit the LLM tool-calling paradigm.

The in vivo to in silico bridge is not a missing tool or model. It is a missing architectural layer for temporal modeling, multi-modal data fusion, and mechanistic inference. Until agent systems can ingest longitudinal behavioral scores, integrate transcriptomic data, quantify uncertainty in dose-response relationships, and navigate multi-objective trade-offs under biological variability, their utility will remain confined to early-stage hit identification. The majority of drug development cost and risk lies beyond that point, in the translation from in vitro activity to in vivo efficacy and safety. Agents that cannot cross this bridge offer limited value to practitioners facing these challenges.
