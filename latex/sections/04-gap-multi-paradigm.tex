\section{Blind Spot 3: Multi-Paradigm, Not Multi-Agent}
\label{sec:multiparadigm}

"Multi-agent" systems in drug discovery AI have multiple LLM-based agents collaborating \citep{seal2025aiagents}. PharmAgents deploys specialized agents for target identification and synthesis. MADD coordinates molecular design and docking. The pattern: an orchestrator LLM decomposes queries, delegates to modules, and synthesizes outputs.

This conflates concepts. "Multi-agent" is multiple LLM instances with different tools. Practitioners need multi-paradigm orchestration: coordinating fundamentally different computational approaches (supervised learning, generative modeling, RL, simulation, optimization) within workflows. Current architectures support the former, not the latter, which is why most real pipelines still require manual glue code.
\subsection{The LLM-as-Orchestrator Assumption}

LLM-centric design treats the language model as central coordinator, invoking external tools via APIs. ChemCrow calls RDKit and PubChem \citep{bran2024chemcrow}. ChatInvent retrieves and synthesizes literature \citep{he2026chatinvent}. This works for text-based reasoning and stateless tools.

The paradigm breaks for tasks like training a multi-task neural network predicting peptide bioactivity across four endpoints using 300 sequences. The workflow: dataset preparation (stratified train-validation-test splits), feature extraction (ESM-2 embeddings), architecture selection, hyperparameter tuning, training with early stopping, validation with confidence intervals.

LLMs cannot orchestrate this via API calls. It requires gradient-based ML with end-to-end control over data loading, loss computation, parameter updates, and checkpointing. Agents do not support supervised learning as a first-class primitive they can configure, execute, monitor, and iterate.

The limitation extends beyond supervised learning. Generative modeling, reinforcement learning, Monte Carlo sampling, molecular dynamics, and Bayesian optimization all require iterative optimization with intermediate states, convergence monitoring, branching logic, resource management (GPU allocation, parallelization, checkpointing), and artifact versioning (models, hyperparameters, trajectories). None fit stateless API calls.

\subsection{The Missing Paradigms}

Absent paradigms define modern drug discovery: supervised learning (bioactivity prediction, toxicity modeling), unsupervised learning (chemical space clustering), generative modeling (de novo design), reinforcement learning (multi-objective optimization), simulation (binding kinetics), and optimization (experimental design).

\begin{table}[htbp]
\centering
\caption{Agent Capability Matrix}
\label{tab:agent-capability}
\small
\begin{tabular}{lccc}
\hline
\textbf{Task Type} & \textbf{Capability} & \textbf{Needs Human Review} & \textbf{Typical Runtime} \\
\hline
Literature review & \checkmark & No & Minutes \\
SMILES generation & \checkmark & Yes & Seconds \\
Docking (small molecules) & \checkmark & Yes & Hours \\
Aggregation prediction & \texttimes & Yes & Days \\
In vivo analysis & \texttimes & Yes & Days to weeks \\
Multi-objective optimization & \texttimes & Yes & Hours to days \\
\hline
\end{tabular}
\end{table}

Across 14 projects, most computational work involved these paradigms, not LLM reasoning. Peptide-receptor classifiers required ESM-2 embeddings, logistic regression, gradient-boosted trees, cross-validation, and AUC-ROC comparison. RNA-seq needed alignment, normalization, differential expression, clustering, and pathway enrichment. Bone formation quantification trained semantic segmentation models. Peptide optimization via RL required reward models, proximal policy optimization, and diversity penalties.

Projects spanned paradigms in integrated pipelines: generative models produced sequences, supervised models predicted bioactivity, Bayesian optimization selected synthesis batches based on uncertainty, experimental results updated training sets, cycles repeated.

Agents cannot express these workflows. Orchestrators call models for inference but cannot train models, incorporate new data, retrain with hyperparameters, validate on test sets, or coordinate generative, predictive, and experimental design algorithms in closed loops. They assume pre-trained models and inference-only tasks, which is the opposite of how discovery actually proceeds.

\subsection{What Multi-Paradigm Orchestration Looks Like}

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{figures/fig4-multi-paradigm-architecture.png}
\caption{From LLM-Centric to Multi-Paradigm Orchestration. Top: Current LLM-centric architecture where a central language model orchestrates all tools through API calls. Bottom: Proposed multi-paradigm architecture where an orchestrator coordinates fundamentally different computational paradigms (ML training pipelines, RL optimization loops, PLM fine-tuning, CV analysis, physics simulations) that execute independently with results aggregated for decision-making.}
\label{fig:multi-paradigm}
\end{figure}

Multi-paradigm architectures treat ML training, RL, simulation, and optimization as core primitives. Practitioners specify workflows declaratively: "Train ensemble bioactivity predictors (XGBoost, random forests, neural nets) with 5-fold cross-validation and hyperparameter tuning. Return Pareto frontier trading AUC-ROC versus calibration error." Agents translate specifications into executable workflow graphs, allocate resources, monitor convergence, and track provenance.

Workflow graphs have nodes (data loading, feature extraction, training, evaluation) and edges (data dependencies). They support parallelization (simultaneous hyperparameter configs), checkpointing (cached intermediate results), and branching (automatic hyperparameter search if validation fails), with clear provenance for every artifact.
Workflow orchestration exists: Apache Airflow, Kubeflow, Nextflow provide task graphs, dependency resolution, resource allocation, and checkpointing. Missing is agent reasoning integration: inspecting results, diagnosing failures, proposing modifications, learning from executions (deprioritizing architectures that overfit).

Interaction should be batch-mode, not chat. Bottlenecks are orchestrating end-to-end analyses, not formulating queries. Small biotechs managing 14 projects need automation: "Analyze eight RNA-seq samples, perform differential expression and pathway enrichment, generate report." Agents intervene only for human decisions (conflicting pathway interpretations).

Human-in-the-loop decision points are explicit and actionable. Agents present Pareto frontiers (accuracy-interpretability trade-offs), candidates with uncertainty estimates. Practitioners select based on context; agents structure decision spaces.

The gap reflects misunderstanding computational drug discovery practice. The field needs systems coordinating paradigms in integrated workflows, batch-mode execution, explicit decision points, and version control for reproducibility, not LLMs chatting.
