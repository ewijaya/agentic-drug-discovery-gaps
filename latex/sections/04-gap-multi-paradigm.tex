\section{Gap 3: Multi-Paradigm, Not Multi-Agent}
\label{sec:multiparadigm}

The term "multi-agent" appears frequently in recent drug discovery AI literature, describing systems where multiple LLM-based agents collaborate to solve complex problems \citep{seal2025aiagents}. PharmAgents deploys specialized agents for target identification, compound selection, and synthesis planning. MADD coordinates agents for molecular design, docking, and activity prediction. The architectural pattern is consistent: an orchestrator LLM decomposes a query into subtasks, delegates them to specialized agent modules, and synthesizes their outputs into a coherent response.

This multi-agent framing, however, conflates two distinct concepts. What the field calls "multi-agent" is primarily multiple instances of LLM reasoning, each with access to different tool sets or knowledge bases. What practitioners actually need is multi-paradigm orchestration: the ability to coordinate fundamentally different computational approaches—supervised learning, generative modeling, reinforcement learning, physical simulation, and constrained optimization—within a single workflow. Current agent architectures support the former but not the latter.

\subsection{The LLM-as-Orchestrator Assumption}

The LLM-centric design pattern treats the language model as the central coordinator and all other computation as external tools invoked via API calls. ChemCrow's GPT-4 orchestrator can call RDKit for molecular property calculations, query PubChem for compound structures, and invoke reaction prediction models \citep{bran2023chemcrow}. ChatInvent's system retrieves literature abstracts, extracts key findings, and synthesizes them into research hypotheses \citep{he2026chatinvent}. These are impressive capabilities when the task naturally decomposes into text-based reasoning and stateless tool invocations.

The paradigm breaks down when the required computation does not fit this pattern. Consider a representative drug discovery task: training a multi-task neural network to predict peptide bioactivity across four endpoints (proliferation, migration, secretion of therapeutic factors, and cytotoxicity) using assay data from 300 proprietary sequences. The workflow involves dataset preparation (train-validation-test splitting with stratification by assay date), feature extraction (computing ESM-2 embeddings for each sequence), model architecture selection (choosing between shared-trunk and task-specific architectures), hyperparameter tuning (learning rate, dropout, batch size), training with early stopping, and validation with confidence interval estimation.

This is not a task an LLM can orchestrate through API calls. It is a gradient-based machine learning workflow requiring end-to-end control over data loading, loss computation, parameter updates, and model checkpointing. The LLM could, perhaps, generate code to execute such a workflow. But current agent architectures do not support supervised learning as a first-class primitive that the agent can configure, execute, monitor, and iterate on based on validation results.

The limitation extends beyond supervised learning. Generative modeling for de novo peptide design requires training variational autoencoders, generative adversarial networks, or autoregressive transformers on domain-specific data. Reinforcement learning for multi-objective sequence optimization involves defining reward functions, training policy networks, and implementing exploration-exploitation strategies to avoid mode collapse. Monte Carlo sampling for peptide landscape exploration requires defining energy functions and acceptance criteria. Molecular dynamics simulations for stability assessment involve force field parameterization and trajectory analysis. Bayesian optimization for experimental design requires surrogate model fitting and acquisition function maximization.

Each of these paradigms has architectural requirements incompatible with LLM tool-calling. They involve iterative optimization with intermediate states, convergence monitoring, and branching logic based on computational results. They require managing computational resources: allocating GPU memory, parallelizing across devices, checkpointing for fault tolerance. They produce artifacts that must be versioned and stored: trained models, hyperparameter configurations, optimization trajectories. None of this fits a stateless API call model.

\subsection{The Missing Paradigms}

The computational paradigms absent from current agent architectures are precisely those that define modern drug discovery. Supervised learning underpins bioactivity prediction, toxicity modeling, and pharmacokinetic property estimation. Unsupervised learning enables chemical space clustering, binding mode classification, and dimensionality reduction for high-dimensional omics data. Generative modeling powers de novo molecular design and peptide sequence generation. Reinforcement learning enables multi-objective optimization with exploration bonuses and curriculum learning. Simulation provides mechanistic insights into binding kinetics, membrane permeability, and metabolic stability. Optimization algorithms, from genetic algorithms to Bayesian methods, guide experimental design and candidate selection.

In our experience across 14 project types, the majority of computational work involved these paradigms, not LLM reasoning. Developing a classifier for peptide-receptor interaction required extracting ESM-2 embeddings, training logistic regression and gradient-boosted tree models, performing cross-validation with hyperparameter search, and selecting the best model via AUC-ROC comparison. Analyzing RNA sequencing data required alignment, normalization, differential expression testing, clustering, and pathway enrichment. Quantifying bone formation from digital tomosynthesis images required training a semantic segmentation model on manually labeled examples, then applying it to thousands of unlabeled images. Optimizing peptides via reinforcement learning required defining a reward model combining bioactivity predictions and stability scores, training a policy network via proximal policy optimization, and implementing diversity penalties to prevent mode collapse.

Each project involved workflows spanning multiple paradigms. Bioactivity prediction used supervised learning. Receptor binding site analysis used unsupervised clustering of docking poses. Peptide generation used reinforcement learning with reward models trained via supervised learning. The workflows were not sequential tool calls but integrated pipelines where the output of one paradigm fed into another. A generative model produced candidate sequences; a supervised model predicted their bioactivity; a Bayesian optimization algorithm selected the next batch for synthesis based on predicted uncertainty; experimental results updated the supervised model's training set; the cycle repeated.

Current agent architectures cannot express these workflows. The orchestrator can call individual models for inference: given a peptide sequence, predict its bioactivity. But it cannot train the bioactivity model, incorporate new experimental data, retrain with updated hyperparameters, and validate on a held-out test set. It cannot coordinate the generative model, the predictor, and the experimental design algorithm in a closed loop. The architecture assumes that all models are pre-trained and that all tasks decompose into inference calls.

\subsection{What Multi-Paradigm Orchestration Looks Like}

A truly multi-paradigm agent architecture would treat machine learning training, reinforcement learning, simulation, and optimization as core computational primitives, not external tools. The practitioner should be able to specify workflows declaratively: "Train an ensemble of bioactivity predictors using XGBoost, random forests, and neural networks. Perform 5-fold cross-validation with hyperparameter tuning. Return the Pareto frontier of models trading off AUC-ROC and calibration error." The agent translates this specification into an executable workflow graph, allocates computational resources, monitors convergence, and returns results with provenance tracking.

The workflow graph abstraction is central. Nodes represent computational steps: data loading, feature extraction, model training, evaluation, visualization. Edges represent data dependencies: the output of feature extraction feeds into model training. The graph supports parallelization: independent hyperparameter configurations can be trained simultaneously. It supports checkpointing: intermediate results can be cached and reused if the workflow is interrupted or modified. It supports branching: if validation performance is unsatisfactory, the workflow can trigger hyperparameter search or architecture changes automatically.

This is not science fiction. Workflow orchestration systems exist in other domains: Apache Airflow for data engineering, Kubeflow for machine learning, Nextflow for bioinformatics. These systems provide the abstractions needed: task graphs, dependency resolution, resource allocation, checkpoint management, and provenance tracking. What is missing is integration with agent reasoning. The agent should be able to inspect workflow results, diagnose failures, propose modifications, and resubmit jobs. It should learn from previous workflow executions: if a particular neural network architecture consistently overfits, it should deprioritize that architecture in future hyperparameter searches.

The interaction model should be batch-mode rather than interactive chat. In our experience, the bottleneck is not formulating individual queries but orchestrating end-to-end analyses involving dozens of computational steps. A small biotech team juggling 14 project types does not have time for interactive debugging sessions with a chatbot. The value proposition is automation: "Analyze all eight RNA sequencing samples from the latest study, perform differential expression analysis, run pathway enrichment, and generate a summary report with figures." The agent should execute this autonomously, intervening only when human decisions are required (e.g., multiple pathway enrichment tools give conflicting results; which interpretation is most biologically plausible?).

Human-in-the-loop decision points should be explicit and actionable. The agent presents a Pareto frontier of trained models, each with different accuracy-interpretability trade-offs. The practitioner selects based on the deployment context: high-stakes clinical decisions require interpretability; internal prioritization can tolerate black-box models if accuracy improves. The agent presents a set of candidate peptides with predicted bioactivity and uncertainty estimates. The practitioner selects based on synthesis feasibility and strategic priorities. The agent does not make these decisions autonomously; it structures the decision space to enable informed human judgment.

The gap between current multi-agent architectures and multi-paradigm orchestration is not merely technical. It reflects a fundamental misunderstanding of how computational drug discovery is practiced. The field does not need multiple LLMs chatting with each other. It needs systems that can coordinate supervised learning, reinforcement learning, simulation, and optimization in integrated workflows, support batch-mode execution for practitioners managing multiple projects, provide explicit human-in-the-loop decision points at key junctures, and version control data, models, and results for reproducibility. Until agent architectures embrace these requirements, they will remain demonstrations rather than tools.
