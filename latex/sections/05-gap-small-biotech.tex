\section{Gap 4: The Small Biotech Reality}
\label{sec:smallbiotech}

ChatInvent's 13-month deployment at AstraZeneca serves as the flagship demonstration of agentic AI in drug discovery \citep{he2026chatinvent}. The system accessed institutional literature databases, high-performance computing clusters, and proprietary compound libraries built over decades. It operated in an environment with specialized teams: medicinal chemists, computational chemists, biologists, and data scientists, each contributing domain expertise. This context, large pharmaceutical company with extensive resources and infrastructure, defines the design assumptions embedded in current agent architectures. But it is not representative of how the majority of drug discovery organizations operate.

Small biotechnology companies, the source of much therapeutic innovation, face fundamentally different constraints. A typical small biotech might have 50 to 100 employees, a single wet lab, limited computational infrastructure, and modest funding that must be carefully allocated across competing priorities. Proprietary datasets are small: hundreds of compounds tested, not millions. Computational work often falls to individuals who are simultaneously designing experiments, analyzing results, and managing projects. The resource profile is 10 to 100 times leaner than large pharmaceutical companies, yet current agent architectures assume the large pharma context.

\subsection{Resource Constraints Current Agents Ignore}

Data scarcity is the first constraint. Large pharmaceutical companies accumulate compound libraries over decades, with hundreds of thousands to millions of molecules tested in standardized assays. This data density enables training high-capacity models with minimal overfitting risk. Small biotechs, especially those developing novel therapeutic modalities like peptides, might have 50 to 500 proprietary sequences with bioactivity measurements. Training data is precious; every assay consumes time, reagents, and synthesis capacity.

Current agent systems assume abundant data. They recommend training deep neural networks with millions of parameters, hyperparameter searches over hundreds of configurations, and ensemble methods combining dozens of models. These approaches are sensible when data is plentiful but disastrous when every training example is hard-won. A deep neural network trained on 100 peptide sequences will overfit catastrophically. Hyperparameter search with 5-fold cross-validation consumes 80% of the data for training, leaving only 20 examples for final evaluation. Ensemble methods that average predictions from 50 models offer no benefit when each model is trained on the same small dataset.

What small biotechs need is data efficiency: models that generalize from few examples, leverage public data through transfer learning, and quantify uncertainty to guide experimental design. Protein language models like ESM-2, pre-trained on millions of natural protein sequences, provide a pathway to transfer learning: extract embeddings from the pre-trained model, then fine-tune a lightweight classifier on the small proprietary dataset. This approach achieves reasonable performance with 50 to 200 training examples, but it requires fine-tuning workflows that current agents do not support.

Active learning offers another data-efficient strategy. Rather than randomly selecting peptides for synthesis and testing, an active learning algorithm identifies sequences where the model's uncertainty is highest or where new data would most improve predictions. In our experience developing peptides with limited testing budgets, active learning reduced the number of assays needed to identify viable candidates by 30 to 40% compared to random sampling. But active learning requires uncertainty quantification, acquisition function optimization, and iterative retraining loops that current agent architectures cannot orchestrate.

Computational constraints compound data scarcity. Small biotechs typically operate with modest computational budgets: a workstation with one or two GPUs, cloud credits used sparingly, and no dedicated HPC cluster. Reinforcement learning for peptide optimization, molecular dynamics simulations for stability assessment, and docking campaigns across thousands of conformations are all computationally expensive. Practitioners must carefully allocate compute resources, prioritizing tasks that offer the highest expected value per GPU-hour.

Current agents provide no support for resource-aware computation. They recommend computationally intensive methods without considering whether the organization has the infrastructure to execute them. They do not optimize workflows for efficiency: parallelizing independent tasks, caching intermediate results, or substituting cheaper approximations when full simulations are unaffordable. A resource-aware agent would propose workflows adapted to available compute: "Given a single GPU and 24 hours, we can either run 100 docking simulations at high precision or 1,000 simulations with a faster but less accurate scoring function. Which trade-off is appropriate for your screening goals?"

Team structure creates a third constraint. At large pharmaceutical companies, computational drug discovery involves specialized roles: machine learning engineers train models, chemists interpret results, biologists design validation experiments, and data engineers maintain infrastructure. At small biotechs, one person often performs all these roles. The practitioner leading a project might design peptides in the morning, run machine learning models in the afternoon, and analyze experimental results in the evening. There is no dedicated data engineer to set up pipelines, no ML ops team to deploy models, and no IT staff to troubleshoot infrastructure issues.

This reality demands tools optimized for generalists, not specialists. The agent must handle the infrastructure complexity: installing dependencies, managing environments, allocating memory, and debugging failures. The practitioner should specify what they want analyzed, not how to set up a Kubernetes cluster or configure a Docker container. Current agents assume someone else handles infrastructure; they generate Python code that assumes libraries are installed, data is in the right format, and compute resources are available. For small biotechs, these assumptions are often false.

\subsection{Transfer Learning and Few-Shot Adaptation}

The solution to data scarcity is transfer learning: leveraging knowledge from large public datasets to improve performance on small proprietary tasks. Protein language models trained on UniProt's hundreds of millions of sequences encode general sequence-structure-function relationships. When fine-tuned on a small dataset of therapeutic peptides with bioactivity labels, these models achieve performance that would require tens of thousands of training examples if trained from scratch.

In practice, effective transfer learning requires more than calling a pre-trained model API. It involves selecting the right pre-trained model (ESM-2, ProtBERT, ProGen), choosing which layers to fine-tune (freezing early layers and adapting later layers is often more data-efficient than fine-tuning all parameters), setting appropriate learning rates (too high causes catastrophic forgetting of pre-trained knowledge; too low leads to slow convergence), and implementing regularization to prevent overfitting. These are architectural and hyperparameter decisions that require experimentation and domain knowledge.

Few-shot learning extends transfer learning to extreme data scarcity. Given 10 to 20 examples of a new peptide class, can a model generalize to unseen sequences? Meta-learning approaches train models to rapidly adapt to new tasks with minimal data. In our experience, few-shot classification using prototypical networks achieved 60 to 70% accuracy on peptide-receptor binding prediction with only 20 training examples per receptor type. This is far from perfect but sufficient for initial prioritization when synthesis capacity is limited.

Current agents provide no pathway to few-shot adaptation. They do not support meta-learning, prototypical networks, or other few-shot methods. They cannot quantify the confidence interval on predictions made from 10 training examples, which is essential for interpreting results. A prediction with 70% accuracy and a 95% confidence interval of plus or minus 5% is actionable; the same accuracy with an interval of plus or minus 30% is not. Uncertainty quantification becomes critical when data is scarce, yet it is absent from current agent capabilities.

Active learning complements transfer learning by intelligently selecting which data to acquire next. The algorithm maintains a model of predictive uncertainty and prioritizes experiments that reduce that uncertainty most efficiently. Acquisition functions like expected improvement or upper confidence bound balance exploitation (synthesizing peptides with high predicted activity) and exploration (synthesizing peptides where the model is uncertain). Over multiple rounds of synthesis, testing, and model updating, active learning converges on high-performing candidates with fewer experiments than random or greedy selection strategies.

In our experience developing bioactivity predictors for peptides, active learning reduced the number of required assays by approximately one-third. The first round synthesized 20 diverse peptides spanning sequence space. The model, trained on these 20 examples, identified regions of high uncertainty. The second round synthesized 15 peptides targeting those regions. By round three, the model's predictions stabilized, and we shifted to exploitation: synthesizing the predicted best candidates. This iterative loop required tight integration between the predictive model, the acquisition function, and experimental feedback, a workflow current agents cannot orchestrate.

\subsection{Batch-Mode Efficiency for Small Teams}

Interactive chat interfaces, central to systems like ChatInvent and Coscientist, assume the practitioner has time for conversational interactions: asking questions, reviewing suggestions, providing feedback. This interaction model works when the agent is one tool among many, used for specific queries. It does not scale when the practitioner is managing 14 project types simultaneously, each generating data that must be processed, analyzed, and synthesized into decisions.

Small biotechs need batch-mode automation. The practitioner specifies a high-level goal: "Analyze the eight RNA sequencing samples from last week's study, identify differentially expressed genes, perform pathway enrichment, and generate a summary report." The agent executes this autonomously, potentially overnight or over a weekend, without requiring interaction. Intervention is needed only when human decisions are required: pathway enrichment identified three plausible mechanisms; which aligns best with prior biological knowledge?

Batch-mode execution requires workflow robustness. If one step fails (e.g., an RNA-seq alignment fails due to memory limits), the agent should either automatically adjust parameters (reduce the number of threads to lower memory usage) or flag the issue for human resolution without losing progress on completed steps. Checkpointing and fault tolerance become essential. The agent must version control inputs, intermediate results, and outputs, enabling reproducibility and debugging.

Parallelization is another efficiency requirement. Given 100 peptide sequences to dock against a protein target, the agent should automatically parallelize the jobs across available compute resources. If eight CPU cores are available, it should launch eight docking jobs simultaneously. If a GPU is available, it should prefer GPU-accelerated docking tools. The practitioner should not manually manage job scheduling; the agent handles this automatically, optimizing for throughput given resource constraints.

Current agents provide minimal support for these capabilities. They are designed for interactive, one-shot queries, not for orchestrating end-to-end analyses that run unsupervised. They do not implement checkpointing, parallelization, or robust error handling. They assume the practitioner will debug failures interactively, which is impractical for overnight batch jobs.

The small biotech reality is not a variant of the large pharma context with fewer resources. It is a fundamentally different operational mode requiring data-efficient learning, transfer learning and few-shot adaptation, active learning for experimental design, batch-mode automation with minimal supervision, and resource-aware workflow optimization. Until agent architectures are designed with these constraints in mind, their applicability beyond well-resourced pharmaceutical companies will remain limited. The agents being built today are optimized for the AstraZenecas of the world. The biotechnology ecosystem, where much innovation happens, is being left behind.
