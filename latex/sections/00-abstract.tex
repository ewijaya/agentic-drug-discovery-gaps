Agentic AI systems have advanced drug discovery automation, with frameworks such as ChatInvent, Coscientist, and ChemCrow demonstrating autonomous synthesis planning, literature mining, and molecular design. However, no systematic evaluation of these frameworks against real-world drug discovery requirements beyond small-molecule, target-based workflows has been conducted. We evaluate six agentic AI frameworks against 15 task classes spanning peptide discovery, in vivo modeling, and resource-constrained settings across five evaluation dimensions: molecular representation coverage, computational paradigm support, data modality integration, resource assumptions, and optimization framework. Our analysis reveals five critical capability gaps: small-molecule bias excluding protein language models and peptide-specific prediction; absent in vivo to in silico bridges for longitudinal, multi-modal animal data; limited computational paradigm support excluding ML training, reinforcement learning, and multi-paradigm coordination; resource assumptions mismatched to small biotech realities; and single-objective optimization ignoring multi-objective trade-offs in safety, efficacy, and stability.
A knowledge-probing experiment across four frontier large language models finds that all
four demonstrate competent peptide reasoning, scoring at or above small-molecule levels
across five pharmaceutical categories (aggregate gap = $-0.115$, all $p > 0.05$),
indicating that peptide expertise exists in foundation models but remains stranded behind
small-molecule-only agent architectures.
From these gaps, we derive design requirements for next-generation frameworks and provide a capability matrix with concrete use cases to guide development toward computational partners that augment practitioner judgment under realistic constraints.
